---
title: "extract finland"
output: html_document
date: "2024-05-28"
---
The initial data are stored in .ods files, one per station. In each file, two daily measures of snow depth (cm) are available, from 1961.01.01 to 2016.12.31

(1) We extract those daily values (the earliest one). Then we compute the annual maxima for each station, on the hydrological year from Y-1/08/01 to Y/07/31, for stations with less than 10% missing values on 'likely maxima period' from Y-1/11/01 to Y/04/30.

(2) We compute a shapefile of the stations, using the coordinates and station names from the netcdf file, and the NUTS-3 shapefile attributes.

---

## Library

```{r, include=FALSE}

library(plyr)
library(tidyverse)
library(tidyr)
library(dplyr)
library(ncdf4)
library(rgdal)
library(sf)
library(readODS)

```

## Parameters

```{r, include=FALSE}

seuil_saison=0.1 
year_int=c(1962:2015) 

```

## Loading files

```{r, include=FALSE}

# list of obs files
csv_name_fin=list.files(file.path("obs", "Finland", "new"), "*.ods", full.names = T)

# NUTS3
shp_nut3 <- st_read(file.path("NUTS","NUTS3_3857.shp"))

```

## Extract daily max 

We extract the earliest snow depth value on the time window set. Then we merge those station data into one single file. 

```{r, include=FALSE}

stat_names <- c("Enontekiö_Kilpisjärvi", "Enontekiö_Nakkala", "Hameenlinna_Lammi_Pappila", "Helsinki_Isosaari", "Helsinki_Kaisaniemi", "Helsinki_Katajaluoto", "Helsinki_Kumpula", "Helsinki_Malmi_airfield", "Ilomantsi_Potsonvaara", "Inari_Angeli_Lintupuoliselka", "Inari_Ivalo_airport", "Jokioinen_Ilmala", "Kankaanpaa_Niinisalo_airfield", "Kauhava_airfield", "Kemi_Tornio_airport", "Kemijarvi_airport", "Kittila_Pokka", "Kouvola_Utti_Lentoportintie", "Kuhmo_Kalliojoki", "Kuusamo_airport", "Kuusamo_Kiutakongas", "Lappeenranta_Lepola", "Ranua_airfield", "Rovaniemi_Apukka", "Salla_Naruska", "Salla_parish", "Salla_Varriotunturi", "Savonlinna_Punkaharju_Laukansaari", "Siikajoki_Ruukki", "Utsjoki_Kevo", "Vaala_Pelso", "Vesanto_parish", "Vierema_Kaarakkala")

# we set up the date interval on which we want to extract data
date_nom=seq(as.Date("1961-01-01"),as.Date("2016-12-31"),by="day")
df_date <- data.frame(matrix(ncol = length(date_nom), nrow = 1))
colnames(df_date)=date_nom

# loop on the stations
sd <- list()
tictoc::tic()
for (i in c(1:length(stat_names))){ 
  
  ods_path <- csv_name_fin[grep(stat_names[i], csv_name_fin)] # file name to read
  csv_doc <- read_ods(ods_path)
  colnames(csv_doc) <- c("Stat", "Year", "Month", "Day", "Time", "Sd")
  
  csv_doc$Month <- sapply(csv_doc$Month, function(x) sprintf('%02d', x)) # setting correct month format
  csv_doc$Day <- sapply(csv_doc$Day, function(x) sprintf('%02d', x)) # setting correct day format
  csv_doc$Sd[which(csv_doc$Sd==-1)] <- rep(0, length(which(csv_doc$Sd==-1))) # setting '-1' (snow depth cm) to '0'
  
  date_nom_obs = apply(csv_doc, 1, function(x) paste(x[2], x[3], x[4], sep="-")) # extracting obs dates

  snow_depth=csv_doc[which(!duplicated(date_nom_obs)),6]
  snow_depth2=mutate_all(as.vector(snow_depth), function(x) as.numeric(x))/100
  df_sd=data.frame(t(snow_depth2))
  colnames(df_sd)=date_nom_obs[!duplicated(date_nom_obs)]
  sd[[i]]=rbind.fill(df_date, df_sd) %>% slice(2) # the obs extracted is put into the "big" dataframe spanning the 1961-2015 period
  
}

tictoc::toc()

obs_fin_day=as.data.frame(do.call(rbind, sd)) # from a list of data frames to a single merged one

# saving dataframe of daily values
save(obs_fin_day, file = file.path("obs", "Finland", "df_fin_day.RData"))

```

## From daily to yearly max 


```{r, include=FALSE}

obs_fin_day <- get(load(file.path("obs", "Finland", "df_fin_day.RData")))

obs_data_year <- list()

# loop on years
i=0
for (y in year_int){

  i=i+1

  ################## FILTERING ON NA RATE ON WINTER SEASON (OCTOBER->APRIL) ##################

    # filtering winter season
  sel_annee <- date_filt(obs_fin_day, month1=c(11:12), month2 = c(1:4), annee = y-1)
  saison_na <- c(sel_annee[[1]],sel_annee[[2]])

    # we save indices of stations with NA exceeding threshold
  tx_na <- apply(obs_fin_day[,saison_na], 1, function(x) length(which(is.na(x)))/length(saison_na)) # vector of na rate per station
  tx_na[which(tx_na>=seuil_saison)] <- NA # stations above thresholds are set as NA

  ################## COMPUTING MAX ON FULL YEAR (AUGUST year-1 -> JULY year+1) ##################

    # filtering year centered on winter y-1 > y
  sel_annee2 <- date_filt(obs_fin_day, month1=c(8:12), month2 = c(1:7), annee = y-1)
  saison_max <- c(sel_annee2[[1]],sel_annee2[[2]])
  
    # allocating NA to timeseries full of NA OR with NA rate too high
  obs_max <- rep(NA,dim(obs_fin_day)[1]) # initialyzing vector of obs max
  finite_value <- which(apply(obs_fin_day[,saison_max],1,function(x) length(which(is.na(x)))<length(x))) # indices of series with at least one value
  obs_max[finite_value] <- apply(obs_fin_day[finite_value,saison_max],1,max,na.rm=TRUE) # replacing original NA with max where possible (at least one value)
  obs_max[is.na(tx_na)]<-NA # replacing with NA where NA rate was above 0.1

  obs_data_year[[i]]<-obs_max

}

fin_max <-  as.data.frame(do.call(cbind, obs_data_year))
colnames(fin_max) <- year_int
save(fin_max, file = file.path("obs", "Finland", "df_fin_year.RData"))

```

## Point shapefile with NUTS3 attribute

```{r, include=FALSE}

coord_csv <- read_ods(file.path("obs", "Finland", "metadata", "FMI_Stations_Coordinates.ods"))
shp_matrix <- matrix(0, nrow = dim(coord_csv)[1], ncol = 3) 
shp_matrix[,1:3]<-c(coord_csv$Latitude,coord_csv$Longitude,coord_csv$`Elevation (in meters)`)
shp_df <- data.frame(shp_matrix)

colnames(shp_df) <- c("lat","lon","alt")
coordinates(shp_df) <- ~lon+lat
proj4string(shp_df) <- CRS("+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0")
sf_fin <- st_as_sf(shp_df) # we transform spatial data frame into shapefile, which is plotting friendly
sf_fin_nut <- st_join(sf_fin, shp_nut3, join = st_nearest_feature) # joining with nut3 id
sf_fin_nut$stat_id <- coord_csv$Name
sf_fin_nut$stat_num <- c(1:33)

st_write(sf_fin_nut, file.path("obs", "Finland", "fin_sf_nut.shp"), delete_layer = TRUE)

```






